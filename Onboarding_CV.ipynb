{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "x9lbO7z-RSzc",
    "outputId": "ff987f68-5191-44f9-ef0d-10ac89aaaedc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_S6gzLURUNd"
   },
   "outputs": [],
   "source": [
    "path_header = \"/content/drive/My Drive/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyR_b54_woZx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import random\n",
    "import sklearn\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "import scipy.stats as st\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "from sklearn.metrics import RocCurveDisplay, auc, roc_curve, confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaNKUFmqO_gu"
   },
   "source": [
    "References:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "https://towardsdatascience.com/pytorch-basics-sampling-samplers-2a0f29f0bf2a\n",
    "\n",
    "https://github.com/bdrad/bdr-cv-onboarding-KellyTrinh/tree/master/bdr-cv-onboarding-KellyTrinh\n",
    "\n",
    "https://biasedml.com/roc-comparison/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoQCTAt0ypzo"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwAOlnoeMDJT"
   },
   "outputs": [],
   "source": [
    "# Load the labels and split into train and test \n",
    "all_labels = pd.read_csv(path_header+\"labels.csv\")\n",
    "test_labels = all_labels[all_labels[\"File Name\"].str.contains(\"Test\")]\n",
    "train_labels = all_labels[all_labels[\"File Name\"].str.contains(\"Train\")]\n",
    "\n",
    "test_labels.to_csv(path_header+\"test_labels.csv\", index=False)\n",
    "train_labels.to_csv(path_header+\"train_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sR0cb55IOUeU"
   },
   "outputs": [],
   "source": [
    "class Mammogram_Dataset(Dataset):\n",
    "  \"\"\"\n",
    "    A custom dataset class for Mammogram files. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "  def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "    '''\n",
    "    Class inherits from PyTorch's Dataset. Acts as custom \n",
    "    Pytorch dataset. Initialize the directory containing the images, \n",
    "    the annotation files, and th transforms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    annotations_file: str\n",
    "        A csv with the file names and labels of the images in the files. \n",
    "    img_dir: str \n",
    "        The directory that contains the images\n",
    "    transform: transform\n",
    "        Modify the features using this transform\n",
    "    target_transform: transform\n",
    "        Modify the labels using this transform\n",
    "    '''\n",
    "      \n",
    "    self.img_labels = pd.read_csv(annotations_file)\n",
    "    self.img_dir = img_dir\n",
    "    self.transform = transform \n",
    "    self.target_transform = target_transform\n",
    "\n",
    "  def __len__(self):\n",
    "    '''\n",
    "    Return the number of samples in the dataset\n",
    "    '''\n",
    "    return len(self.img_labels)\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "      # get image\n",
    "      img_name = path_header +\"numpyz_files/\" + self.img_labels[\"File Name\"][idx]\n",
    "      image = np.load(img_name)[\"arr_0\"]\n",
    "      image = np.array(image).astype(\"float32\")\n",
    "      \n",
    "      # get density\n",
    "      density = int(self.img_labels[\"Density\"][idx])\n",
    "      density = np.array(density).astype(\"float32\")\n",
    "\n",
    "      if self.transform:\n",
    "            image = self.transform(image)\n",
    "          \n",
    "      image = image.type(torch.float32)\n",
    "      density = torch.tensor(density, dtype=torch.int64)\n",
    "\n",
    "      return image, density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4rnSgWnOiIz"
   },
   "outputs": [],
   "source": [
    "tensor_transform = transforms.Compose([transforms.ToTensor()])\n",
    "test_dataset = Mammogram_Dataset(annotations_file=path_header+\"test_labels.csv\", img_dir=path_header+\"numpyz_files\", transform=tensor_transform)\n",
    "train_dataset = Mammogram_Dataset(annotations_file=path_header+\"train_labels.csv\", img_dir=path_header + \"numpyz_files\", transform=tensor_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "RtYNV-U9NtCy",
    "outputId": "1d97bbc8-d337-4a6b-ad3e-fa4e3e1dc815"
   },
   "outputs": [],
   "source": [
    "# Display 10 mammogram images \n",
    "\n",
    "test_iter =  iter(test_dataloader)\n",
    "features, labels = test_iter.next()\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "rows = 2\n",
    "columns = 5\n",
    "\n",
    "num_img = 10\n",
    "for idx in range(num_img):\n",
    "  img = features[idx].squeeze()\n",
    "  label = labels[idx].item()\n",
    "  fig.add_subplot(rows, columns, idx + 1)\n",
    "  plt.imshow(img, cmap=\"gray\")\n",
    "  plt.title(\"Breast density:\" + str(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTv8l5GUyjcx"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVU4ThbcYHj_"
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(num_classes=2)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJ6ILqE1w5tW"
   },
   "outputs": [],
   "source": [
    "def train_model_val(model, criterion, optimizer, num_epochs=25):\n",
    "    '''\n",
    "    Return the trained model and print training and validation loss after each epoch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "      model: Pytorch model\n",
    "          Starting model\n",
    "      criterion: Pytorch criterion\n",
    "          Compute a gradient using the given loss function\n",
    "      optimizer: Pytorch optimizer\n",
    "          Performs updates\n",
    "      num_epochs: int\n",
    "          Number of iterations to perform\n",
    "    '''\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    train_dataset2, val_dataset = random_split(train_dataset, (round(len(train_dataset) * .80), round(len(train_dataset) * .20)))\n",
    "    train_loader = DataLoader(dataset=train_dataset2, shuffle=True, batch_size=1)\n",
    "    val_loader = DataLoader(dataset=val_dataset, shuffle=False, batch_size=1)\n",
    "\n",
    "\n",
    "    dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "    dataset_sizes = {'train': len(train_loader), 'val': len(val_loader)}\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "hARma6qRYIAr",
    "outputId": "29c5a20b-0257-4ea1-dd45-7ef432a8f124"
   },
   "outputs": [],
   "source": [
    "train_model_val(model, criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9T9D4dfdaKZ"
   },
   "source": [
    "## Test and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "YHtYPKL_RjGX",
    "outputId": "df74f6d0-2ef6-4113-a971-450a4e387f1a"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# set model to prediction mode\n",
    "model.eval()\n",
    "\n",
    "# keep track of output probabilities and true labels for model evaluation\n",
    "y_predict = []\n",
    "y_probs = []\n",
    "y_true = []\n",
    "\n",
    "# not training so don't need to calculate the gradients\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "      \n",
    "      # send data to device\n",
    "      images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "      # perform prediction \n",
    "      outputs = model(images)\n",
    "      # pass through sigmoid layer\n",
    "      predicted_probs = torch.sigmoid(outputs.data)\n",
    "      # choose the class with the highest energy as the prediction\n",
    "      _, predicted = torch.max(predicted_probs, 1)\n",
    "\n",
    "      # for later model evaluation\n",
    "      y_predict.extend(predicted.cpu().numpy())\n",
    "      y_true.extend(labels.cpu().numpy())\n",
    "      y_probs.extend(predicted_probs.cpu().numpy())\n",
    "\n",
    "      # keeping track of number of correct predictions\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrjokvzyPuR0"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred):\n",
    "    '''\n",
    "    y_pred: predictions array\n",
    "    y_true: true labels array\n",
    "    ------\n",
    "    Display a confusion matrix. \n",
    "    '''\n",
    "    cm = sklearn.metrics.confusion_matrix(y_true, y_pred, labels = [0, 1])\n",
    "    df_cm = pd.DataFrame(cm, [\"True 0\", \"True 1\"], [\"Predicted 0\", \"Predicted 1\"])\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16})\n",
    "\n",
    "def roc_curve(y_prob, y_true):\n",
    "    '''\n",
    "    y_prob: (NUMPY ARRAY) predicted probabilities\n",
    "    y_true: (NUMPY ARRAY) true labels\n",
    "    ------\n",
    "    Display ROC curve.\n",
    "    '''\n",
    "\n",
    "    y_prob = np.array(y_prob)\n",
    "\n",
    "    # calculate values to draw the roc curve \n",
    "    fpr, tpr, _ = sklearn.metrics.roc_curve(y_true, np.array(y_prob)[:,1])\n",
    "    # calculate the area under the curve value \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # plot the roc curve with the auc value \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "            lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def bootstrap_auc(y_true, y_pred, random_state=24, percentile=95):\n",
    "    '''\n",
    "    y_pred: (NUMPY ARRAY) predictions\n",
    "    y_true: (NUMPY ARRAY) true labels\n",
    "    random_state: (INT) random seed used when bootstrapping\n",
    "    percentile: (INT) which percentile to bootstrap (i.e 95, 80, etc.)\n",
    "    ------\n",
    "    Bootstrap the AUC values and visualize the histogram \n",
    "    of the bootstrapped values. Return the lower bound\n",
    "    and upper bound of the confidence interval around the \n",
    "    bootstrap AUC values. \n",
    "    '''\n",
    "    \n",
    "    # bootstrap the AUC value \n",
    "\n",
    "    # set the seed\n",
    "    seed = 100\n",
    "    random.seed(seed)\n",
    "    num_pred = len(y_pred)\n",
    "\n",
    "    num_bootstrap = 1000 \n",
    "\n",
    "    # list of bootstrapped AUC values\n",
    "    bootstrapped_auc = np.array([])\n",
    "\n",
    "    # perform bootstrap \n",
    "    for i in range(num_bootstrap):\n",
    "      # sample with replacement from the indices of the y prediction list\n",
    "      random_idx = random.choices(range(num_pred), k = num_pred)\n",
    "\n",
    "      b_y_pred = np.array(y_pred)[random_idx]\n",
    "      b_y_true = np.array(y_true)[random_idx]\n",
    "\n",
    "      # generate new AUC value from the bootstrapped y prediction values \n",
    "      fpr, tpr, _ = sklearn.metrics.roc_curve(b_y_true, b_y_pred)\n",
    "      roc_auc = auc(fpr, tpr)\n",
    "\n",
    "      bootstrapped_auc = np.append(bootstrapped_auc, roc_auc)\n",
    "\n",
    "    # plot the histogram \n",
    "    plt.title(\"Histogram of bootstrapped AUC values\")    \n",
    "    plt.hist(bootstrapped_auc, bins=50);\n",
    "\n",
    "    lower_bound = np.percentile(bootstrapped_auc, (100 - percentile) / 2)\n",
    "    upper_bound = np.percentile(bootstrapped_auc, (100 - percentile) / 2 + percentile)\n",
    "\n",
    "    return lower_bound, upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "-uYaBJVw43Uj",
    "outputId": "30a7cbb0-82fe-41d7-e1ca-71711ed596d2"
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_true, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "FAc3hkod7Eze",
    "outputId": "41e3c71f-dcdf-4a57-93b9-402df731e97a"
   },
   "outputs": [],
   "source": [
    "roc_curve(y_probs, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "TWH3SVzJ7Y03",
    "outputId": "e836870c-9a35-4619-a3e4-62007aecce0a"
   },
   "outputs": [],
   "source": [
    "bootstrap_auc(y_true, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJngN2X4Pub8"
   },
   "outputs": [],
   "source": [
    "# create the Delong test \n",
    "\n",
    "def z_score(var_A, var_B, covar_AB, auc_A, auc_B):\n",
    "    '''\n",
    "    Calculate the z-score.\n",
    "    Let auc_a = auc of first classifier; \n",
    "    auc_b = auc of second classifier. \n",
    "\n",
    "    z-score = (auc_b - auc_b) / Var(auc_a - auc_b)\n",
    "    where \n",
    "    Var(auc_a - auc_b) = Var(auc_a) + Var(auc_b) - 2*Cov(auc_a, auc_b)\n",
    "    '''\n",
    "    return (auc_A - auc_B)/((var_A + var_B - 2*covar_AB)**(.5))\n",
    "\n",
    "def auc(X, Y):\n",
    "    '''\n",
    "    Calculates the auc which is equal to the Mann-Whitney statistic \n",
    "    applied to the two samples X and Y. \n",
    "    '''\n",
    "    return 1/(len(X)*len(Y)) * sum([self.kernel(x, y) for x in X for y in Y])\n",
    "\n",
    "def kernel(X, Y):\n",
    "    '''\n",
    "    Helper function to calculate the auc.\n",
    "    If X = Y return 1/2, if X < Y return 0, else return 1.\n",
    "    '''\n",
    "    return .5 if Y==X else int(Y < X)\n",
    "\n",
    "def structural_components(X, Y):\n",
    "    '''\n",
    "    Estimate the variance V (arising from the finite test set) \n",
    "    and the covariance C (due to the common test set).\n",
    "    '''\n",
    "    V10 = [1/len(Y) * sum([kernel(x, y) for y in Y]) for x in X]\n",
    "    V01 = [1/len(X) * sum([kernel(x, y) for x in X]) for y in Y]\n",
    "    return V10, V01\n",
    "      \n",
    "def get_S_entry(V_A, V_B, auc_A, auc_B):\n",
    "    '''\n",
    "    Using the theory on generalized U-statistics, \n",
    "    the function get_S_entry(V_A, V_B, auc_A, auc_B): \n",
    "    generates an estimated variance-covariance matrix S.\n",
    "    '''\n",
    "    return 1/(len(V_A)-1) * sum([(a-auc_A)*(b-auc_B) for a,b in zip(V_A, V_B)])\n",
    "\n",
    "def group_preds_by_label(preds, actual):\n",
    "  \n",
    "    X = [p for (p, a) in zip(preds, actual) if a] # prediction with actual label = 1\n",
    "    Y = [p for (p, a) in zip(preds, actual) if not a] # prediction with actual label = 0\n",
    "    return X, Y\n",
    "  \n",
    "def DeLongTest(y_probs, y_true):\n",
    "    '''\n",
    "    Performs a DeLong test to test if the different in the auc value \n",
    "    of the classifier (classifer A) with the auc of a random classifier \n",
    "    (classifier B) is statistically significant. \n",
    "    '''\n",
    "    \n",
    "    preds_A = np.array(y_probs)[:,1]\n",
    "    actual = y_true\n",
    "    preds_B = np.random.rand(preds_A.shape[0]) # predictions from a random classifier\n",
    "    X_A, Y_A = group_preds_by_label(preds_A, actual) # model A\n",
    "    X_B, Y_B = group_preds_by_label(preds_B, actual) # model B\n",
    "\n",
    "    V_A10, V_A01 = structural_components(X_A, Y_A) \n",
    "    V_B10, V_B01 = structural_components(X_B, Y_B) \n",
    "    auc_A = auc(X_A, Y_A)\n",
    "    auc_B = auc(X_B, Y_B)\n",
    "\n",
    "    # Compute entries of covariance matrix S (covar_AB = covar_BA)\n",
    "    var_A = (get_S_entry(V_A10, V_A10, auc_A, auc_A) * 1/len(V_A10)\n",
    "            + get_S_entry(V_A01, V_A01, auc_A, auc_A) * 1/len(V_A01))\n",
    "    var_B = (get_S_entry(V_B10, V_B10, auc_B, auc_B) * 1/len(V_B10)\n",
    "            + get_S_entry(V_B01, V_B01, auc_B, auc_B) * 1/len(V_B01))\n",
    "    covar_AB = (get_S_entry(V_A10, V_B10, auc_A, auc_B) * 1/len(V_A10)\n",
    "                + get_S_entry(V_A01, V_B01, auc_A, auc_B) * 1/len(V_A01))\n",
    "    # Two tailed test\n",
    "    z = z_score(var_A, var_B, covar_AB, auc_A, auc_B)\n",
    "    p = st.norm.sf(abs(z))*2\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "BWinifZM9C2z",
    "outputId": "75199732-05c9-41e9-efa8-94dda5f15ba1"
   },
   "outputs": [],
   "source": [
    "DeLongTest(y_probs, y_true) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
